{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff87cf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting weasyprint\n",
      "  Downloading WeasyPrint-52.4-py3-none-any.whl (365 kB)\n",
      "\u001b[K     |████████████████████████████████| 365 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cssselect2>=0.1\n",
      "  Downloading cssselect2-0.4.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: cffi>=0.6 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (1.14.5)\n",
      "Requirement already satisfied: setuptools>=39.2.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (49.6.0.post20210108)\n",
      "Requirement already satisfied: html5lib>=0.999999999 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (1.1)\n",
      "Requirement already satisfied: Pillow>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (8.1.0)\n",
      "Collecting cairocffi>=0.9.0\n",
      "  Downloading cairocffi-1.2.0.tar.gz (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 4.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tinycss2>=1.0.0\n",
      "  Downloading tinycss2-1.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting Pyphen>=0.9.1\n",
      "  Downloading Pyphen-0.10.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting CairoSVG>=2.4.0\n",
      "  Downloading CairoSVG-2.5.2-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from CairoSVG>=2.4.0->weasyprint) (0.6.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from cffi>=0.6->weasyprint) (2.20)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from cssselect2>=0.1->weasyprint) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from html5lib>=0.999999999->weasyprint) (1.15.0)\n",
      "Building wheels for collected packages: cairocffi\n",
      "  Building wheel for cairocffi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cairocffi: filename=cairocffi-1.2.0-py3-none-any.whl size=89545 sha256=dd7528dda4354e7c2f2b6153cc145dcd56749209fbd74fdd930625d8f908cfe5\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/37/f0/7e/f92a9b82bde542ad7e50f6f91b368f84fab2cf1176ba265aa5\n",
      "Successfully built cairocffi\n",
      "Installing collected packages: tinycss2, cssselect2, cairocffi, Pyphen, CairoSVG, weasyprint\n",
      "Successfully installed CairoSVG-2.5.2 Pyphen-0.10.0 cairocffi-1.2.0 cssselect2-0.4.1 tinycss2-1.1.0 weasyprint-52.4\n"
     ]
    }
   ],
   "source": [
    "! pip install weasyprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57257ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webrtcvad\n",
      "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 3.0 MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: webrtcvad\n",
      "  Building wheel for webrtcvad (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp36-cp36m-linux_x86_64.whl size=30065 sha256=d530a5cec7dc007831193568038d3efa1626eacb8bade4ad012f613880950d2c\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ba/22/1c/d4e9707bbb27d469c384efc4263d8c7125219c1f088937289c\n",
      "Successfully built webrtcvad\n",
      "Installing collected packages: webrtcvad\n",
      "Successfully installed webrtcvad-2.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7003cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webrtcvad\n",
    "import collections\n",
    "import contextlib\n",
    "import sys\n",
    "import wave\n",
    "import numpy as np \n",
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "import os \n",
    "import cv2\n",
    "from weasyprint import HTML\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184f89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02437176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57917399",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data='s3://sagemaker-us-east-2-808810818304/model/HP_new_model_Serial.tar.gz', role=role,\n",
    "                             framework_version='1.6.0',\n",
    "                             py_version='py3',\n",
    "                             entry_point='inference.py')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c766aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1.3.1. Ignoring framework/algorithm version: 1.6.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.deserializers import NumpyDeserializer\n",
    "from sagemaker.serializers import NumpySerializer\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m4.xlarge',\n",
    "                                     initial_instance_count=1,\n",
    "                                     accelerator_type='ml.eia1.large' , \n",
    "                                Serializer = NumpySerializer() , \n",
    "                                Deserializer = NumpyDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0fa599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pytorch-inference-eia-2021-07-23-15-25-51-547'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0c256",
   "metadata": {},
   "source": [
    "## Only test no valid \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0b48003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "s3 = boto3.resource('s3') # assumes credentials & configuration are handled outside python in .aws directory or environment variables\n",
    "\n",
    "def download_s3_folder(bucket_name, s3_folder, local_dir=None):\n",
    "    \"\"\"\n",
    "    Download the contents of a folder directory\n",
    "    Args:\n",
    "        bucket_name: the name of the s3 bucket\n",
    "        s3_folder: the folder path in the s3 bucket\n",
    "        local_dir: a relative or absolute directory path in the local file system\n",
    "    \"\"\"\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    for obj in bucket.objects.filter(Prefix=s3_folder):\n",
    "        target = obj.key if local_dir is None \\\n",
    "            else os.path.join(local_dir, os.path.relpath(obj.key, s3_folder))\n",
    "        if not os.path.exists(os.path.dirname(target)):\n",
    "            os.makedirs(os.path.dirname(target))\n",
    "        if obj.key[-1] == '/':\n",
    "            continue\n",
    "        bucket.download_file(obj.key, target)\n",
    "download_s3_folder('object-detection-grad-proj', 'fpdf', local_dir='mnt/acessfpdf/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09ea2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_s3_folder('object-detection-grad-proj', 'fpdf', local_dir='mnt/acessfpdf/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de056cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "frame = cv2.imread('test_phs/test_no.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "411a98fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'mat' (pos 2) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-12f5d252c7aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'mat' (pos 2) not found"
     ]
    }
   ],
   "source": [
    "cv2.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e955dc4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"Failure in Face Detection\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 126, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 216, in _default_transform_fn\n    prediction = self._predict_fn(data, model)\n  File \"/opt/ml/model/code/inference.py\", line 235, in predict_fn\n    raise Exception(\"Failure in Face Detection\")\nException: Failure in Face Detection\n\". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-eia-2021-06-03-08-16-43-463 in account 808810818304 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2afd0dd779d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"Failure in Face Detection\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 126, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 216, in _default_transform_fn\n    prediction = self._predict_fn(data, model)\n  File \"/opt/ml/model/code/inference.py\", line 235, in predict_fn\n    raise Exception(\"Failure in Face Detection\")\nException: Failure in Face Detection\n\". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-eia-2021-06-03-08-16-43-463 in account 808810818304 for more information."
     ]
    }
   ],
   "source": [
    "try :\n",
    "    predictor.predict(frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b69eb6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'NG1AY562BJYEE3WV',\n",
       "  'HostId': 'mRXDqBgOUSy9q908AknjZZKFxoLBYeHXXTOlngMhN6Trry8klySvjnXWQFCD1au0nFHZFFdWbxc=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'mRXDqBgOUSy9q908AknjZZKFxoLBYeHXXTOlngMhN6Trry8klySvjnXWQFCD1au0nFHZFFdWbxc=',\n",
       "   'x-amz-request-id': 'NG1AY562BJYEE3WV',\n",
       "   'date': 'Thu, 29 Apr 2021 00:17:49 GMT',\n",
       "   'x-amz-bucket-region': 'us-east-2',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'IsTruncated': False,\n",
       " 'Contents': [{'Key': 'student_1/',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 51, 39, tzinfo=tzlocal()),\n",
       "   'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'Size': 0,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/Photo on 05-01-2021 at 6.41 AM.jpg',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 52, 13, tzinfo=tzlocal()),\n",
       "   'ETag': '\"48c396e4c7966048de590197158d10a1\"',\n",
       "   'Size': 193920,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/Photo on 05-01-2021 at 6.42 AM #2.jpg',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 52, 13, tzinfo=tzlocal()),\n",
       "   'ETag': '\"6b91dec293f321aec092938017783ee4\"',\n",
       "   'Size': 193702,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/Photo on 05-01-2021 at 6.42 AM #3.jpg',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 52, 13, tzinfo=tzlocal()),\n",
       "   'ETag': '\"b2d1079ea4c3c0d0403d92049465f411\"',\n",
       "   'Size': 213239,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/Photo on 05-01-2021 at 6.42 AM #4.jpg',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 52, 13, tzinfo=tzlocal()),\n",
       "   'ETag': '\"c1ba85a42e13495c6776087e8cd1a9ad\"',\n",
       "   'Size': 213407,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/Photo on 05-01-2021 at 6.42 AM #5.jpg',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 52, 13, tzinfo=tzlocal()),\n",
       "   'ETag': '\"c75a03abc648439c53261b672f033431\"',\n",
       "   'Size': 214239,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/Photo on 05-01-2021 at 6.42 AM #6.jpg',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 52, 13, tzinfo=tzlocal()),\n",
       "   'ETag': '\"af1579102d1343ff64a7cafde9890574\"',\n",
       "   'Size': 214205,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/Photo on 05-01-2021 at 6.42 AM #7.jpg',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 52, 13, tzinfo=tzlocal()),\n",
       "   'ETag': '\"3c3c1163d41c53bf1264ac70654c79e3\"',\n",
       "   'Size': 213827,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/Photo on 05-01-2021 at 6.42 AM #8.jpg',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 52, 13, tzinfo=tzlocal()),\n",
       "   'ETag': '\"fd8de8b71084540863f664a7395ed879\"',\n",
       "   'Size': 213975,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/Photo on 05-01-2021 at 6.42 AM #9.jpg',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 52, 13, tzinfo=tzlocal()),\n",
       "   'ETag': '\"65a26764b9858d746ff03785efae3bd2\"',\n",
       "   'Size': 213827,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/Photo on 05-01-2021 at 6.43 AM.jpg',\n",
       "   'LastModified': datetime.datetime(2021, 2, 21, 3, 52, 13, tzinfo=tzlocal()),\n",
       "   'ETag': '\"28bc298363c84b991bfb728520bb032f\"',\n",
       "   'Size': 216205,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  {'Key': 'student_1/stud_1.wav',\n",
       "   'LastModified': datetime.datetime(2021, 4, 18, 2, 40, 42, tzinfo=tzlocal()),\n",
       "   'ETag': '\"1ecfffefedc5a3ab4741627e0aa11ca9\"',\n",
       "   'Size': 3932254,\n",
       "   'StorageClass': 'STANDARD'}],\n",
       " 'Name': 'lamda-grad-proj-bucket',\n",
       " 'Prefix': 'student_1',\n",
       " 'MaxKeys': 100,\n",
       " 'EncodingType': 'url',\n",
       " 'KeyCount': 12}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3 \n",
    "s3 = boto3.client(\"s3\")\n",
    "response = s3.list_objects_v2(\n",
    "        Bucket='lamda-grad-proj-bucket',\n",
    "        Prefix ='student_1',\n",
    "        MaxKeys=100 )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2836e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "image = cv2.imread('mnt/acess/___student1/IMG-7322.JPG')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eae4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _npy_dumps(data):\n",
    "    \"\"\"\n",
    "    Serializes a numpy array into a stream of npy-formatted bytes.\n",
    "    \"\"\"\n",
    "    from six import BytesIO\n",
    "    import numpy as np\n",
    "    buffer = BytesIO()\n",
    "    np.save(buffer, data)\n",
    "    return buffer.getvalue()\n",
    "\n",
    "def _npy_loads(data):\n",
    "    \"\"\"\n",
    "    Deserializes npy-formatted bytes into a numpy array\n",
    "    \"\"\"\n",
    "    from six import BytesIO\n",
    "    import numpy as np\n",
    "    stream = BytesIO(data)\n",
    "    return np.load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bbc3384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.679047, 33.019577, 12.135437], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Prediction within notebook\n",
    "'''\n",
    "predictor.predict((image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51448e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25537920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.532928466796875, 4.305908203125, 2.4061126708984375]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.invoke_endpoint(EndpointName=predictor.endpoint,\n",
    "                                   ContentType='application/x-npy',\n",
    "                                   Body=_npy_dumps(image))\n",
    "for x in response['Body'].iter_lines() : \n",
    "    z = x\n",
    "json.loads(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a3774e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image (file_name) :\n",
    "    supported_formats = ['png', 'jpg', 'jpeg' , 'JPG'] \n",
    "    for supported_format in supported_formats : \n",
    "        if supported_format in file_name :\n",
    "            return True\n",
    "    return False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6ac49",
   "metadata": {},
   "source": [
    "## Pose Cheating Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efb38cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pose_score(list_of_pose):\n",
    "    score = []\n",
    "    for i , x in enumerate(list_of_pose) : \n",
    "        if i == 0 :\n",
    "            if x == 'center' : \n",
    "                score.append(0)\n",
    "            else : \n",
    "                score.append(1)\n",
    "        if i > 0 : \n",
    "            j = i - 1 \n",
    "            if x == 'center' : \n",
    "                score.append(0)\n",
    "            else : \n",
    "                if list_of_pose[j] == 'center':\n",
    "                    score.append(1)\n",
    "                if list_of_pose[j] == x:\n",
    "                    score.append(1)\n",
    "                if list_of_pose[j] != x and list_of_pose[j] != 'center' :\n",
    "                    score.append(2)\n",
    "    return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eded650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_final_score (V_list_of_pose , H_list_of_pose , isreporting = False ) : \n",
    "    score_v = np.array(calc_pose_score(V_list_of_pose))\n",
    "    score_h = np.array(calc_pose_score(H_list_of_pose))\n",
    "    if isreporting :\n",
    "        return score_v , score_h\n",
    "    else : \n",
    "        return np.sum(score_v+ score_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed8b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cheating_pose (V_list_of_pose , H_list_of_pose,threshold) : \n",
    "    cheating_score= get_pose_final_score(V_list_of_pose , H_list_of_pose)\n",
    "    cheating_rate = cheating_score/ len(H_list_of_pose)\n",
    "    cheating_result = cheating_rate > threshold \n",
    "    print('Cheating Score :',cheating_score)\n",
    "    print('cheating rate :' ,cheating_rate  )\n",
    "    return cheating_score ,cheating_rate ,  cheating_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a84bce87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ['center','up','center','down','up']\n",
    "J = ['left','right','center','center','right']\n",
    "len(X),len(J)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db88e736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cheating Score : 8\n",
      "cheating rate : 1.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 1.6, True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cheating_pose(X,J,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "713e1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_directions(yaw , pitch ) :\n",
    "    dir_yaw = '' \n",
    "    dir_pitch = '' \n",
    "    if -15 < yaw < 15 : \n",
    "        dir_yaw = 'center'\n",
    "    elif yaw < -15 : \n",
    "        dir_yaw = 'right'\n",
    "    elif yaw > 15 : \n",
    "        dir_yaw = 'left'\n",
    "    if -15 < pitch < 15 : \n",
    "        dir_pitch = 'center'\n",
    "    elif pitch < -15 : \n",
    "        dir_pitch = 'down'\n",
    "    elif pitch > 15 : \n",
    "        dir_pitch = 'up'\n",
    "    return dir_yaw , dir_pitch \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98bbf3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('center', 'up')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaw , pitch = transfer_to_directions(-14 , 40 )\n",
    "yaw , pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2bcddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_pose_list=[]\n",
    "yaw_pose_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b05bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cheating_pose_detection_run(path , threshold) : \n",
    "    pitch_pose_list=[]\n",
    "    yaw_pose_list=[]\n",
    "    count = 0\n",
    "    for img_path in os.listdir(path) :\n",
    "        if is_image (img_path) :\n",
    "            count = count +1 \n",
    "            frame = cv2.imread(path+'/'+img_path)\n",
    "            pitch , yaw, _ = predictor.predict(frame)\n",
    "            yaw , pitch = transfer_to_directions(yaw , pitch)\n",
    "            pitch_pose_list.append(pitch)\n",
    "            yaw_pose_list.append(yaw)\n",
    "            print (count , 'Images Processed')\n",
    "    print (yaw_pose_list , pitch_pose_list)\n",
    "    return is_cheating_pose(pitch_pose_list,yaw_pose_list,threshold)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fe1e1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Images Processed\n",
      "2 Images Processed\n",
      "3 Images Processed\n",
      "4 Images Processed\n",
      "5 Images Processed\n",
      "6 Images Processed\n",
      "7 Images Processed\n",
      "8 Images Processed\n",
      "['center', 'left', 'center', 'right', 'center', 'left', 'center', 'right'] ['down', 'center', 'center', 'center', 'down', 'center', 'up', 'center']\n",
      "Cheating Score : 7\n",
      "cheating rate : 0.875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, 0.875, True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path= os.getcwd()+'/test_phs'\n",
    "is_cheating_pose_detection_run(path,0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906a7a8",
   "metadata": {},
   "source": [
    "## VOICE ACTIVITY DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7934d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad = webrtcvad.Vad(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d80dc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Frame(object):\n",
    "\n",
    "    def __init__(self, bytes, timestamp, duration):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "    offset = 0\n",
    "    timestamp = 0.0\n",
    "    duration = (float(n) / sample_rate) / 2.0\n",
    "    while offset + n < len(audio):\n",
    "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
    "        timestamp += duration\n",
    "        offset += n\n",
    "\n",
    "def read_wave(path):\n",
    "\n",
    "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "        num_channels = wf.getnchannels()\n",
    "        assert num_channels == 1\n",
    "        sample_width = wf.getsampwidth()\n",
    "        assert sample_width == 2\n",
    "        sample_rate = wf.getframerate()\n",
    "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
    "        pcm_data = wf.readframes(wf.getnframes())\n",
    "        return pcm_data, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e202cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAD_detection(wav_file_path,threshold) : \n",
    "    sound = AudioSegment.from_wav(wav_file_path)\n",
    "    sound = sound.set_frame_rate(48000)\n",
    "    sound = sound.set_channels(1)\n",
    "    sound = sound.set_sample_width(2)\n",
    "    sound.export(wav_file_path, format=\"wav\")\n",
    "    audio, sample_rate = read_wave(wav_file_path)\n",
    "    frames = frame_generator(10, audio, sample_rate)\n",
    "    frames = list(frames)\n",
    "    is_speech=[]\n",
    "    for frame in frames:\n",
    "          is_speech.append(vad.is_speech(frame.bytes, sample_rate))\n",
    "    is_speech_np = np.array(is_speech)\n",
    "    return np.sum(is_speech) , (is_speech_np.sum()>threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb0a7e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file_path = 'mnt/acess/___student1/stud_1.wav'\n",
    "threshold = 200 \n",
    "VAD_detection(wav_file_path,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34a944",
   "metadata": {},
   "source": [
    "## Object Detection -YOLO-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b399842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(imagePath, yoloPath, confidenceNeeded=0.5, threshold=0.3):\n",
    "\n",
    "    # load the COCO class labels our YOLO model was trained on\n",
    "    labelsPath = os.path.sep.join([yoloPath, \"coco.names\"])\n",
    "    LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "    # initialize a list of colors to represent each possible class label\n",
    "    np.random.seed(42)\n",
    "    COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "        dtype=\"uint8\")\n",
    "\n",
    "    # derive the paths to the YOLO weights and model configuration\n",
    "    weightsPath = os.path.sep.join([yoloPath, \"yolov3.weights\"])\n",
    "    configPath = os.path.sep.join([yoloPath, \"yolov3.cfg\"])\n",
    "\n",
    "    # load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "    print(\"[INFO] loading YOLO from disk...\")\n",
    "    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "    # load our input image and grab its spatial dimensions\n",
    "    image = cv2.imread(imagePath)\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # determine only the *output* layer names that we need from YOLO\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    # construct a blob from the input image and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "    # associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "        swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layerOutputs = net.forward(ln)\n",
    "    end = time.time()\n",
    "\n",
    "    # show timing information on YOLO\n",
    "    print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))\n",
    "\n",
    "    # initialize our lists of detected bounding boxes, confidences, and\n",
    "    # class IDs, respectively\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence (i.e., probability) of\n",
    "            # the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter out weak predictions by ensuring the detected\n",
    "            # probability is greater than the minimum probability\n",
    "            if confidence > confidenceNeeded:\n",
    "                # scale the bounding box coordinates back relative to the\n",
    "                # size of the image, keeping in mind that YOLO actually\n",
    "                # returns the center (x, y)-coordinates of the bounding\n",
    "                # box followed by the boxes' width and height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # use the center (x, y)-coordinates to derive the top and\n",
    "                # and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "                # update our list of bounding box coordinates, confidences,\n",
    "                # and class IDs\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "    # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "    # boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confidenceNeeded,\n",
    "        threshold)\n",
    "\n",
    "    # ensure at least one detection exists\n",
    "    result=[]\n",
    "    if len(idxs) > 0:\n",
    "        for i in idxs.flatten():\n",
    "            result.append([LABELS[classIDs[i]], confidences[i]])\n",
    "\n",
    "            \n",
    "\n",
    "    # return the output\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a76201e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_palgarism_objects_txt(all_objects_that_can_be_detected , mode ):\n",
    "    f = open(all_objects_that_can_be_detected, \"r\")\n",
    "    list_of_palgarism_objects = [] \n",
    "    objects = f.read().split(\"\\n\") \n",
    "    for obj in objects : \n",
    "        if \"*\" in obj :\n",
    "            list_of_palgarism_objects.append((''.join([i for i in obj if not i.isdigit()])).replace('*','').replace('-',''))\n",
    "    if mode == 1 : \n",
    "        f = open(\"palgarism_objects_to_detect.txt\", \"w\")\n",
    "        for obj in list_of_palgarism_objects : \n",
    "            f.write(obj + \"\\n\")\n",
    "        f.close()\n",
    "    if mode == 2 : \n",
    "        return list_of_palgarism_objects\n",
    "    else : \n",
    "        return -1 \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e54b723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_output_yolo(yolo_output , palgarism_objects) : \n",
    "    yolo_filtered_output = [] \n",
    "    for obj , conf in yolo_output : \n",
    "        if obj in palgarism_objects : \n",
    "            yolo_filtered_output.append([obj ,conf ])\n",
    "    return yolo_filtered_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d12cb4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backpack',\n",
       " 'handbag',\n",
       " 'suitcase',\n",
       " 'tvmonitor',\n",
       " 'laptop',\n",
       " 'cell phone',\n",
       " 'book']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_palgarism_objects_txt(\"palgarism_objects.txt\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c1569d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolo(image_path) : \n",
    "    output=detect(image_path,os.getcwd())\n",
    "    yolo_filtered = filter_output_yolo(output ,\n",
    "                        get_palgarism_objects_txt(\"palgarism_objects.txt\",2))\n",
    "    return yolo_filtered\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "882b3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cheating_yolo(image_path) :\n",
    "    cheating_objs = []\n",
    "    yolo_out = run_yolo(image_path)\n",
    "    if len(yolo_out) > 0 :\n",
    "        for item in yolo_out : \n",
    "            cheating_objs.append(item[0])  \n",
    "        return cheating_objs , True \n",
    "    else:\n",
    "        return 'None' , False \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbd2c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 1.057860 seconds\n"
     ]
    }
   ],
   "source": [
    "path_image = \"test_4.jpeg\" \n",
    "yolo_obj_list ,yolo_obj_result  =  is_cheating_yolo(path_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d54c861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cell phone'], True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_obj_list ,yolo_obj_result \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4bf7d",
   "metadata": {},
   "source": [
    "### Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d050e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb807ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cheating_pose_yolo_detection_run(path , threshold) : \n",
    "    pitch_pose_list=[]\n",
    "    yaw_pose_list=[]\n",
    "    all_yolo_obj_list=[]\n",
    "    all_yolo_obj_result=[]\n",
    "    count = 0\n",
    "    #for img_path in os.listdir(path) old not sorted:\n",
    "    for img_path in sorted(os.listdir(path) , key=natural_keys):\n",
    "        if is_image (img_path) :\n",
    "            count = count +1 \n",
    "            frame = cv2.imread(path+'/'+img_path)\n",
    "            pitch , yaw, _ = predictor.predict(frame)\n",
    "            yaw , pitch = transfer_to_directions(yaw , pitch)\n",
    "            pitch_pose_list.append(pitch)\n",
    "            yaw_pose_list.append(yaw)\n",
    "            yolo_obj_list ,yolo_obj_result  = is_cheating_yolo(path+'/'+img_path)\n",
    "            all_yolo_obj_list.append(yolo_obj_list)\n",
    "            all_yolo_obj_result .append(yolo_obj_result)\n",
    "            print (count , 'Images Processed')\n",
    "    print (yaw_pose_list , pitch_pose_list)\n",
    "    cheating_score ,cheating_rate ,  cheating_result = is_cheating_pose(pitch_pose_list,yaw_pose_list,threshold)\n",
    "    return cheating_score ,cheating_rate ,  cheating_result ,all_yolo_obj_list,all_yolo_obj_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c2a57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cheating_detection_complete_run(path , score_threshold , sound_threshold) : \n",
    "    detected_frames_list = []\n",
    "    sound_result_list = [] \n",
    "    cheating_score ,cheating_rate ,  cheating_result ,all_yolo_obj_list,all_yolo_obj_result = is_cheating_pose_yolo_detection_run(path , score_threshold)\n",
    "    for file in os.listdir(path) : \n",
    "        if 'wav' in file : \n",
    "            wav_file_path = path+'/'+file\n",
    "            detected_frames , sound_result = VAD_detection(wav_file_path,sound_threshold)\n",
    "            detected_frames_list.append(detected_frames)\n",
    "            sound_result_list.append(sound_result)\n",
    "    return cheating_score ,cheating_rate ,cheating_result ,all_yolo_obj_list,np.sum(all_yolo_obj_result) ,detected_frames_list,sound_result_list \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed12c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.947625 seconds\n",
      "1 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.937614 seconds\n",
      "2 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.934223 seconds\n",
      "3 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.940207 seconds\n",
      "4 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.936347 seconds\n",
      "5 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.940550 seconds\n",
      "6 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.943097 seconds\n",
      "7 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.936971 seconds\n",
      "8 Images Processed\n",
      "['left', 'left', 'center', 'right', 'right', 'center', 'center', 'center'] ['center', 'center', 'center', 'center', 'center', 'up', 'down', 'down']\n",
      "Cheating Score : 8\n",
      "cheating rate : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8,\n",
       " 1.0,\n",
       " True,\n",
       " ['None', 'None', 'None', 'None', 'None', 'None', 'None', ['cell phone']],\n",
       " 1,\n",
       " [327],\n",
       " [True])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cheating_detection_complete_run('mnt/acess/___student2',0.8,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d46a2",
   "metadata": {},
   "source": [
    "## Integration & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "env = Environment(loader=FileSystemLoader('.'))\n",
    "template = env.get_template(\"report.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878787da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cheating_detection_reporting_and_responsing(path,score_threshold,frames_threshold): \n",
    "    cheating_score ,cheating_rate ,cheating_result ,all_yolo_obj_list,all_yolo_obj_result ,detected_frames_list\\\n",
    "    ,sound_result_list = cheating_detection_complete_run(path,score_threshold,frames_threshold)\n",
    "\n",
    "    if cheating_result or all_yolo_obj_result or np.array(sound_result_list).sum() : \n",
    "        return True\n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "str([['cell phone'], 'None', 'None', 'None', 'None', 'None', 'None', 'None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cheating_detection_reporting_and_responsing(path,0.8,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05255dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/acess/student_1/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file_path = '/mnt/acess/student_1/stud_1.wav'\n",
    "wav_file_path.replace(wav_file_path.split('/')[-1],'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51dbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
