{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "downtown-referral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: weasyprint in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (52.4)\n",
      "Requirement already satisfied: cairocffi>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (1.2.0)\n",
      "Requirement already satisfied: setuptools>=39.2.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (49.6.0.post20210108)\n",
      "Requirement already satisfied: tinycss2>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (1.1.0)\n",
      "Requirement already satisfied: CairoSVG>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (2.5.2)\n",
      "Requirement already satisfied: Pyphen>=0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (0.10.0)\n",
      "Requirement already satisfied: html5lib>=0.999999999 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (1.1)\n",
      "Requirement already satisfied: Pillow>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (8.1.0)\n",
      "Requirement already satisfied: cssselect2>=0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (0.4.1)\n",
      "Requirement already satisfied: cffi>=0.6 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from weasyprint) (1.14.5)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from CairoSVG>=2.4.0->weasyprint) (0.6.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from cffi>=0.6->weasyprint) (2.20)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from cssselect2>=0.1->weasyprint) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from html5lib>=0.999999999->weasyprint) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install weasyprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "maritime-wellington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webrtcvad\n",
      "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 694 kB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: webrtcvad\n",
      "  Building wheel for webrtcvad (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp36-cp36m-linux_x86_64.whl size=30065 sha256=5f5b68fbab13d078b4d65d4cce7a7998ab25cb17df17040dad33f777f433ba5e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ba/22/1c/d4e9707bbb27d469c384efc4263d8c7125219c1f088937289c\n",
      "Successfully built webrtcvad\n",
      "Installing collected packages: webrtcvad\n",
      "Successfully installed webrtcvad-2.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "impaired-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thermal-terry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import webrtcvad\n",
    "import collections\n",
    "import contextlib\n",
    "import sys\n",
    "import wave\n",
    "import numpy as np \n",
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "import os \n",
    "import cv2\n",
    "from weasyprint import HTML\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legendary-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tropical-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alive-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data='s3://sagemaker-us-east-2-808810818304/model/HPmodel_Serial.tar.gz', role=role,\n",
    "                             framework_version='1.6.0',\n",
    "                             py_version='py3',\n",
    "                             entry_point='inference.py')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "loaded-election",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1.3.1. Ignoring framework/algorithm version: 1.6.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.deserializers import NumpyDeserializer\n",
    "from sagemaker.serializers import NumpySerializer\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m4.xlarge',\n",
    "                                     initial_instance_count=1,\n",
    "                                     accelerator_type='ml.eia1.large' , \n",
    "                                Serializer = NumpySerializer() , \n",
    "                                Deserializer = NumpyDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fifty-mumbai",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pytorch-inference-eia-2021-04-16-00-15-00-416'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-kidney",
   "metadata": {},
   "source": [
    "## Only test no valid \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "effective-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "image = cv2.imread('test_phs/Photo on 05-01-2021 at 6.42 AM #7.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ranking-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _npy_dumps(data):\n",
    "    \"\"\"\n",
    "    Serializes a numpy array into a stream of npy-formatted bytes.\n",
    "    \"\"\"\n",
    "    from six import BytesIO\n",
    "    import numpy as np\n",
    "    buffer = BytesIO()\n",
    "    np.save(buffer, data)\n",
    "    return buffer.getvalue()\n",
    "\n",
    "def _npy_loads(data):\n",
    "    \"\"\"\n",
    "    Deserializes npy-formatted bytes into a numpy array\n",
    "    \"\"\"\n",
    "    from six import BytesIO\n",
    "    import numpy as np\n",
    "    stream = BytesIO(data)\n",
    "    return np.load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "going-former",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.090973 , -40.535374 ,   0.8360443], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Prediction within notebook\n",
    "'''\n",
    "predictor.predict((image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "statutory-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "becoming-joshua",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-6.090972900390625, -40.53537368774414, 0.8360443115234375]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.invoke_endpoint(EndpointName=predictor.endpoint,\n",
    "                                   ContentType='application/x-npy',\n",
    "                                   Body=_npy_dumps(image))\n",
    "for x in response['Body'].iter_lines() : \n",
    "    z = x\n",
    "json.loads(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "heated-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image (file_name) :\n",
    "    supported_formats = ['png', 'jpg', 'jpeg' , 'JPG'] \n",
    "    for supported_format in supported_formats : \n",
    "        if supported_format in file_name :\n",
    "            return True\n",
    "    return False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-wallpaper",
   "metadata": {},
   "source": [
    "## Pose Cheating Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "starting-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pose_score(list_of_pose):\n",
    "    score = []\n",
    "    for i , x in enumerate(list_of_pose) : \n",
    "        if i == 0 :\n",
    "            if x == 'center' : \n",
    "                score.append(0)\n",
    "            else : \n",
    "                score.append(1)\n",
    "        if i > 0 : \n",
    "            j = i - 1 \n",
    "            if x == 'center' : \n",
    "                score.append(0)\n",
    "            else : \n",
    "                if list_of_pose[j] == 'center':\n",
    "                    score.append(1)\n",
    "                if list_of_pose[j] == x:\n",
    "                    score.append(1)\n",
    "                if list_of_pose[j] != x and list_of_pose[j] != 'center' :\n",
    "                    score.append(2)\n",
    "    return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bulgarian-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_final_score (V_list_of_pose , H_list_of_pose , isreporting = False ) : \n",
    "    score_v = np.array(calc_pose_score(V_list_of_pose))\n",
    "    score_h = np.array(calc_pose_score(H_list_of_pose))\n",
    "    if isreporting :\n",
    "        return score_v , score_h\n",
    "    else : \n",
    "        return np.sum(score_v+ score_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "numeric-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cheating_pose (V_list_of_pose , H_list_of_pose,threshold) : \n",
    "    cheating_score= get_pose_final_score(V_list_of_pose , H_list_of_pose)\n",
    "    cheating_rate = cheating_score/ len(H_list_of_pose)\n",
    "    cheating_result = cheating_rate > threshold \n",
    "    print('Cheating Score :',cheating_score)\n",
    "    print('cheating rate :' ,cheating_rate  )\n",
    "    return cheating_score ,cheating_rate ,  cheating_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "funded-candy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ['center','up','center','down','up']\n",
    "J = ['left','right','center','center','right']\n",
    "len(X),len(J)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "lightweight-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cheating Score : 8\n",
      "cheating rate : 1.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 1.6, True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cheating_pose(X,J,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "expanded-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_directions(yaw , pitch ) :\n",
    "    dir_yaw = '' \n",
    "    dir_pitch = '' \n",
    "    if -15 < yaw < 15 : \n",
    "        dir_yaw = 'center'\n",
    "    elif yaw < -15 : \n",
    "        dir_yaw = 'right'\n",
    "    elif yaw > 15 : \n",
    "        dir_yaw = 'left'\n",
    "    if -15 < pitch < 15 : \n",
    "        dir_pitch = 'center'\n",
    "    elif pitch < -15 : \n",
    "        dir_pitch = 'down'\n",
    "    elif pitch > 15 : \n",
    "        dir_pitch = 'up'\n",
    "    return dir_yaw , dir_pitch \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "incorrect-service",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('center', 'up')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaw , pitch = transfer_to_directions(-14 , 40 )\n",
    "yaw , pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "confused-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_pose_list=[]\n",
    "yaw_pose_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "saved-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cheating_pose_detection_run(path , threshold) : \n",
    "    pitch_pose_list=[]\n",
    "    yaw_pose_list=[]\n",
    "    count = 0\n",
    "    for img_path in os.listdir(path) :\n",
    "        if is_image (img_path) :\n",
    "            count = count +1 \n",
    "            frame = cv2.imread(path+'/'+img_path)\n",
    "            pitch , yaw, _ = predictor.predict(frame)\n",
    "            yaw , pitch = transfer_to_directions(yaw , pitch)\n",
    "            pitch_pose_list.append(pitch)\n",
    "            yaw_pose_list.append(yaw)\n",
    "            print (count , 'Images Processed')\n",
    "    print (yaw_pose_list , pitch_pose_list)\n",
    "    return is_cheating_pose(pitch_pose_list,yaw_pose_list,threshold)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "organizational-writing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Images Processed\n",
      "2 Images Processed\n",
      "3 Images Processed\n",
      "4 Images Processed\n",
      "5 Images Processed\n",
      "6 Images Processed\n",
      "7 Images Processed\n",
      "8 Images Processed\n",
      "['center', 'left', 'center', 'right', 'center', 'left', 'center', 'right'] ['down', 'center', 'center', 'center', 'down', 'center', 'up', 'center']\n",
      "Cheating Score : 7\n",
      "cheating rate : 0.875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, 0.875, True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path= os.getcwd()+'/test_phs'\n",
    "is_cheating_pose_detection_run(path,0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-subdivision",
   "metadata": {},
   "source": [
    "## VOICE ACTIVITY DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "unusual-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad = webrtcvad.Vad(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "wrapped-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Frame(object):\n",
    "\n",
    "    def __init__(self, bytes, timestamp, duration):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "    offset = 0\n",
    "    timestamp = 0.0\n",
    "    duration = (float(n) / sample_rate) / 2.0\n",
    "    while offset + n < len(audio):\n",
    "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
    "        timestamp += duration\n",
    "        offset += n\n",
    "\n",
    "def read_wave(path):\n",
    "\n",
    "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "        num_channels = wf.getnchannels()\n",
    "        assert num_channels == 1\n",
    "        sample_width = wf.getsampwidth()\n",
    "        assert sample_width == 2\n",
    "        sample_rate = wf.getframerate()\n",
    "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
    "        pcm_data = wf.readframes(wf.getnframes())\n",
    "        return pcm_data, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "charged-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAD_detection(wav_file_path,threshold) : \n",
    "    sound = AudioSegment.from_wav(wav_file_path)\n",
    "    sound = sound.set_frame_rate(48000)\n",
    "    sound = sound.set_channels(1)\n",
    "    sound = sound.set_sample_width(2)\n",
    "    sound.export(wav_file_path, format=\"wav\")\n",
    "    audio, sample_rate = read_wave(wav_file_path)\n",
    "    frames = frame_generator(10, audio, sample_rate)\n",
    "    frames = list(frames)\n",
    "    is_speech=[]\n",
    "    for frame in frames:\n",
    "          is_speech.append(vad.is_speech(frame.bytes, sample_rate))\n",
    "    is_speech_np = np.array(is_speech)\n",
    "    return np.sum(is_speech) , (is_speech_np.sum()>threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "freelance-eagle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file_path = 'student1/stud_1.wav'\n",
    "threshold = 200 \n",
    "VAD_detection(wav_file_path,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-tourism",
   "metadata": {},
   "source": [
    "## Object Detection -YOLO-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "planned-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(imagePath, yoloPath, confidenceNeeded=0.5, threshold=0.3):\n",
    "\n",
    "    # load the COCO class labels our YOLO model was trained on\n",
    "    labelsPath = os.path.sep.join([yoloPath, \"coco.names\"])\n",
    "    LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "    # initialize a list of colors to represent each possible class label\n",
    "    np.random.seed(42)\n",
    "    COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "        dtype=\"uint8\")\n",
    "\n",
    "    # derive the paths to the YOLO weights and model configuration\n",
    "    weightsPath = os.path.sep.join([yoloPath, \"yolov3.weights\"])\n",
    "    configPath = os.path.sep.join([yoloPath, \"yolov3.cfg\"])\n",
    "\n",
    "    # load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "    print(\"[INFO] loading YOLO from disk...\")\n",
    "    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "    # load our input image and grab its spatial dimensions\n",
    "    image = cv2.imread(imagePath)\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # determine only the *output* layer names that we need from YOLO\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    # construct a blob from the input image and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "    # associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "        swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layerOutputs = net.forward(ln)\n",
    "    end = time.time()\n",
    "\n",
    "    # show timing information on YOLO\n",
    "    print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))\n",
    "\n",
    "    # initialize our lists of detected bounding boxes, confidences, and\n",
    "    # class IDs, respectively\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence (i.e., probability) of\n",
    "            # the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter out weak predictions by ensuring the detected\n",
    "            # probability is greater than the minimum probability\n",
    "            if confidence > confidenceNeeded:\n",
    "                # scale the bounding box coordinates back relative to the\n",
    "                # size of the image, keeping in mind that YOLO actually\n",
    "                # returns the center (x, y)-coordinates of the bounding\n",
    "                # box followed by the boxes' width and height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # use the center (x, y)-coordinates to derive the top and\n",
    "                # and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "                # update our list of bounding box coordinates, confidences,\n",
    "                # and class IDs\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "    # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "    # boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confidenceNeeded,\n",
    "        threshold)\n",
    "\n",
    "    # ensure at least one detection exists\n",
    "    result=[]\n",
    "    if len(idxs) > 0:\n",
    "        for i in idxs.flatten():\n",
    "            result.append([LABELS[classIDs[i]], confidences[i]])\n",
    "\n",
    "            \n",
    "\n",
    "    # return the output\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "subject-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_palgarism_objects_txt(all_objects_that_can_be_detected , mode ):\n",
    "    f = open(all_objects_that_can_be_detected, \"r\")\n",
    "    list_of_palgarism_objects = [] \n",
    "    objects = f.read().split(\"\\n\") \n",
    "    for obj in objects : \n",
    "        if \"*\" in obj :\n",
    "            list_of_palgarism_objects.append((''.join([i for i in obj if not i.isdigit()])).replace('*','').replace('-',''))\n",
    "    if mode == 1 : \n",
    "        f = open(\"palgarism_objects_to_detect.txt\", \"w\")\n",
    "        for obj in list_of_palgarism_objects : \n",
    "            f.write(obj + \"\\n\")\n",
    "        f.close()\n",
    "    if mode == 2 : \n",
    "        return list_of_palgarism_objects\n",
    "    else : \n",
    "        return -1 \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "differential-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_output_yolo(yolo_output , palgarism_objects) : \n",
    "    yolo_filtered_output = [] \n",
    "    for obj , conf in yolo_output : \n",
    "        if obj in palgarism_objects : \n",
    "            yolo_filtered_output.append([obj ,conf ])\n",
    "    return yolo_filtered_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "french-olympus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backpack',\n",
       " 'handbag',\n",
       " 'suitcase',\n",
       " 'tvmonitor',\n",
       " 'laptop',\n",
       " 'cell phone',\n",
       " 'book']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_palgarism_objects_txt(\"palgarism_objects.txt\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "moderate-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolo(image_path) : \n",
    "    output=detect(image_path,os.getcwd())\n",
    "    yolo_filtered = filter_output_yolo(output ,\n",
    "                        get_palgarism_objects_txt(\"palgarism_objects.txt\",2))\n",
    "    return yolo_filtered\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "trained-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cheating_yolo(image_path) :\n",
    "    cheating_objs = []\n",
    "    yolo_out = run_yolo(image_path)\n",
    "    if len(yolo_out) > 0 :\n",
    "        for item in yolo_out : \n",
    "            cheating_objs.append(item[0])  \n",
    "        return cheating_objs , True \n",
    "    else:\n",
    "        return 'None' , False \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "suitable-feedback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 1.078174 seconds\n"
     ]
    }
   ],
   "source": [
    "path_image = \"test_4.jpeg\" \n",
    "yolo_obj_list ,yolo_obj_result  =  is_cheating_yolo(path_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "quarterly-working",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cell phone'], True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_obj_list ,yolo_obj_result \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-brunei",
   "metadata": {},
   "source": [
    "### Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "residential-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "apart-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cheating_pose_yolo_detection_run(path , threshold) : \n",
    "    pitch_pose_list=[]\n",
    "    yaw_pose_list=[]\n",
    "    all_yolo_obj_list=[]\n",
    "    all_yolo_obj_result=[]\n",
    "    count = 0\n",
    "    #for img_path in os.listdir(path) old not sorted:\n",
    "    for img_path in sorted(os.listdir(path) , key=natural_keys):\n",
    "        if is_image (img_path) :\n",
    "            count = count +1 \n",
    "            frame = cv2.imread(path+'/'+img_path)\n",
    "            pitch , yaw, _ = predictor.predict(frame)\n",
    "            yaw , pitch = transfer_to_directions(yaw , pitch)\n",
    "            pitch_pose_list.append(pitch)\n",
    "            yaw_pose_list.append(yaw)\n",
    "            yolo_obj_list ,yolo_obj_result  = is_cheating_yolo(path+'/'+img_path)\n",
    "            all_yolo_obj_list.append(yolo_obj_list)\n",
    "            all_yolo_obj_result .append(yolo_obj_result)\n",
    "            print (count , 'Images Processed')\n",
    "    print (yaw_pose_list , pitch_pose_list)\n",
    "    cheating_score ,cheating_rate ,  cheating_result = is_cheating_pose(pitch_pose_list,yaw_pose_list,threshold)\n",
    "    return cheating_score ,cheating_rate ,  cheating_result ,all_yolo_obj_list,all_yolo_obj_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "selective-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cheating_detection_complete_run(path , score_threshold , sound_threshold) : \n",
    "    detected_frames_list = []\n",
    "    sound_result_list = [] \n",
    "    cheating_score ,cheating_rate ,  cheating_result ,all_yolo_obj_list,all_yolo_obj_result = is_cheating_pose_yolo_detection_run(path , score_threshold)\n",
    "    for file in os.listdir(path) : \n",
    "        if 'wav' in file : \n",
    "            wav_file_path = path+'/'+file\n",
    "            detected_frames , sound_result = VAD_detection(wav_file_path,sound_threshold)\n",
    "            detected_frames_list.append(detected_frames)\n",
    "            sound_result_list.append(sound_result)\n",
    "    return cheating_score ,cheating_rate ,cheating_result ,all_yolo_obj_list,np.sum(all_yolo_obj_result) ,detected_frames_list,sound_result_list \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "prime-pasta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.974235 seconds\n",
      "1 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.963979 seconds\n",
      "2 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.957979 seconds\n",
      "3 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.959109 seconds\n",
      "4 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.965252 seconds\n",
      "5 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.967860 seconds\n",
      "6 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.967581 seconds\n",
      "7 Images Processed\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.963053 seconds\n",
      "8 Images Processed\n",
      "['center', 'left', 'center', 'right', 'center', 'left', 'center', 'right'] ['down', 'center', 'center', 'center', 'down', 'center', 'up', 'center']\n",
      "Cheating Score : 7\n",
      "cheating rate : 0.875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " 0.875,\n",
       " True,\n",
       " [['cell phone'], 'None', 'None', 'None', 'None', 'None', 'None', 'None'],\n",
       " 1,\n",
       " [327],\n",
       " [True])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cheating_detection_complete_run(path,0.8,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-leader",
   "metadata": {},
   "source": [
    "## Integration & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "env = Environment(loader=FileSystemLoader('.'))\n",
    "template = env.get_template(\"report.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cheating_detection_reporting_and_responsing(path,score_threshold,frames_threshold): \n",
    "    cheating_score ,cheating_rate ,cheating_result ,all_yolo_obj_list,all_yolo_obj_result ,detected_frames_list\\\n",
    "    ,sound_result_list = cheating_detection_complete_run(path,score_threshold,frames_threshold)\n",
    "    report_df = pd.DataFrame ({\n",
    "        'cheating score':cheating_score ,\n",
    "        'cheating rate':cheating_rate ,\n",
    "        'Pose cheating result' : cheating_result ,\n",
    "        'objects_detected' :str(all_yolo_obj_list),\n",
    "        'Num objects detected' : all_yolo_obj_result,\n",
    "        'Num Frames of Human sound':detected_frames_list,\n",
    "        'Sound Detection result':sound_result_list },index = ['Student']).T\n",
    "    template_vars = {\"title\" : \"Final Report\",\n",
    "                     \"table\": report_df.to_html()}\n",
    "    html_out = template.render(template_vars)\n",
    "    HTML(string=html_out).write_pdf(\"report.pdf\")\n",
    "    if cheating_result or all_yolo_obj_result or np.array(sound_result_list).sum() : \n",
    "        return True\n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "str([['cell phone'], 'None', 'None', 'None', 'None', 'None', 'None', 'None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "cheating_detection_reporting_and_responsing(path,0.8,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-jenny",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
